{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOYVivZO9mrs"
      },
      "outputs": [],
      "source": [
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"/content/AD.zip\", 'r') as zObject:\n",
        "\n",
        "    # Extracting all the members of the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall(\n",
        "        path=\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"/content/TD.zip\", 'r') as zObject:\n",
        "\n",
        "    # Extracting all the members of the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall(\n",
        "        path=\"/content\")"
      ],
      "metadata": {
        "id": "epKzw1oQ3jQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "repeat for combination (C3, Cz) and (C3,C4)"
      ],
      "metadata": {
        "id": "asXZEa7pSrjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pywavelets\n",
        "import pandas as pd\n",
        "ns =11\n",
        "ori_data = dict()\n",
        "mod_data = dict()\n",
        "ori_data1 = dict()\n",
        "mod_data1 = dict()\n",
        "# function to count subject\n",
        "def subjectCounter(i):\n",
        "    return 'subject0{}'.format(i)\n",
        "for i in range(1, ns):\n",
        "    data_path = '/content/AD/AD/AV{:02d}.xlsx'.format(i)\n",
        "    subj = subjectCounter(i)\n",
        "    ori_data[subj] =  pd.read_excel(data_path, usecols=['C4','Cz'])\n",
        "    mod_data[subj] = {}\n",
        "    mod_data[subj]['raw_EEG'] = ori_data[subj]\n",
        "for i in range(1, ns):\n",
        "    data_path = '/content/TD/TD{:02d}.xlsx'.format(i)\n",
        "    subj = subjectCounter(i)\n",
        "    ori_data1[subj] =  pd.read_excel(data_path, usecols=['C4','Cz'])\n",
        "    mod_data1[subj] = {}\n",
        "    mod_data1[subj]['raw_EEG'] = ori_data1[subj]"
      ],
      "metadata": {
        "id": "xWvcunpqSVft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztl9RQqPn1FG"
      },
      "outputs": [],
      "source": [
        "from scipy import signal\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data[subj] = {}\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data1[subj] = {}\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data[subj]['raw_EEG'] = ori_data[subj]\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data1[subj]['raw_EEG'] = ori_data1[subj]\n",
        "from scipy.signal import butter, lfilter\n",
        "from scipy.signal import freqz\n",
        "from scipy.signal import butter, filtfilt\n",
        "def butter_bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut/nyq\n",
        "    high = highcut/nyq\n",
        "    b,a = butter(order, [low, high], btype='band')\n",
        "    y = filtfilt(b, a, signal)\n",
        "    return y\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data[subj]['raw_EEG'] = mod_data[subj]['raw_EEG'].T\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data1[subj]['raw_EEG'] = mod_data1[subj]['raw_EEG'].T\n",
        "import pywt\n",
        "from scipy import signal\n",
        "import numpy as np\n",
        "def dwt_bandpass_one_subject(data, subj, lowcut, highcut, fs, interval=None):\n",
        "    data[subj]['EEG_filtered'] = {}\n",
        "    temp_raw_EEG = data[subj]['raw_EEG']\n",
        "    temp_band = 'temp_band'  # Replace 'your_desired_key' with the actual key in your dictionary\n",
        "    if interval is not None:\n",
        "        startband = np.arange(lowcut, highcut, step=interval)\n",
        "        for start in startband:\n",
        "            temp_band = \"{:02d}_{:02d}\".format(start, start+interval)\n",
        "            data[subj]['EEG_filtered']['temp_band'] = {}\n",
        "            data[subj]['EEG_filtered']['temp_band'] = dwt_bandpass_filter(temp_raw_EEG, start, start+interval, fs)\n",
        "    else:\n",
        "        temp_band = \"{:02d}_{:02d}\".format(lowcut, highcut)\n",
        "        data[subj]['EEG_filtered']['temp_band'] = {}\n",
        "        data[subj]['EEG_filtered']['temp_band'] = dwt_bandpass_filter(temp_raw_EEG, lowcut, highcut, fs)\n",
        "def dwt_bandpass_filter(data, lowcut, highcut, fs):\n",
        "    coeffs = pywt.wavedec(data, 'db4', level=3)\n",
        "    bands = [(0.5, 32)]\n",
        "    subband_filters = []\n",
        "    for low, high in bands:\n",
        "        b, a = signal.butter(5, [low/(fs/2), high/(fs/2)], btype='bandpass')\n",
        "    for i in range(1, 4):\n",
        "        for j in range(0, 3):\n",
        "            coeffs[i][j] = signal.filtfilt(b, a, coeffs[i][j])\n",
        "    filtered_data = pywt.waverec(coeffs, 'db4')\n",
        "    return filtered_data\n",
        "lowcut=8\n",
        "highcut=32\n",
        "fs = 500\n",
        "temp_band = 'temp_band'\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    dwt_bandpass_one_subject(mod_data, subj, lowcut, highcut, fs, interval = None)\n",
        "lowcut=8\n",
        "highcut=32\n",
        "fs = 500\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    dwt_bandpass_one_subject(mod_data1, subj, lowcut, highcut, fs, interval = None)\n",
        "def butter_bandpass_one_subject(data, subj, lowcut, highcut, fs, interval=None):\n",
        "    data[subj]['filtered'] = {}\n",
        "    temp_raw_EEG = data[subj]['EEG_filtered']['temp_band']\n",
        "    if interval is not None:\n",
        "        startband = np.arange(lowcut, highcut, step = interval)\n",
        "        for start in startband:\n",
        "            band = \"{:02d}_{:02d}\".format(start, start+interval)\n",
        "            data[subj]['filtered'][band] = {}\n",
        "            data[subj]['filtered'][band]['EEG_all'] = butter_bandpass_filter(temp_raw_EEG, start, start+interval, fs, order=5)\n",
        "    else:\n",
        "        band = \"{:02d}_{:02d}\".format(lowcut, highcut)\n",
        "        data[subj]['EEG_filtered'][band]['EEG_all'] = butter_bandpass_filter(temp_raw_EEG, lowcut, highcut, fs)\n",
        "lowcut=8\n",
        "highcut=32\n",
        "fs = 500\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    butter_bandpass_one_subject(mod_data, subj, lowcut, highcut, fs, interval = 4)\n",
        "lowcut=8\n",
        "highcut=32\n",
        "fs = 500\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    butter_bandpass_one_subject(mod_data1, subj, lowcut, highcut, fs, interval = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da3BFJjaV1C5"
      },
      "outputs": [],
      "source": [
        "ASD = 'ASD'\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    print('Processing ', subj)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        temp_EEG_all = mod_data[subj]['filtered'][band]['EEG_all']\n",
        "        temp_ASD = []\n",
        "        for j in range(0,61):\n",
        "            subset_size = 1000\n",
        "            temp_ASD.append(temp_EEG_all[:,j * subset_size:(j+1) * subset_size])\n",
        "        mod_data[subj]['filtered'][band]['ASD'] = np.array(temp_ASD)\n",
        "TD = 'TD'\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    print('Processing ', subj)\n",
        "    for band in mod_data1[subj]['filtered'].keys():\n",
        "        temp_EEG_all = mod_data1[subj]['filtered'][band]['EEG_all']\n",
        "        temp_TD = []\n",
        "        for j in range(0,61):\n",
        "            subset_size = 1000\n",
        "            temp_TD.append(temp_EEG_all[:,j * subset_size:(j+1) * subset_size])\n",
        "        mod_data1[subj]['filtered'][band]['TD'] = np.array(temp_TD)\n",
        "def split_EEG_one_class(EEG_one_class, percent_train=0.8):\n",
        "    n = EEG_one_class.shape[0]\n",
        "    n_tr = round(n*percent_train)\n",
        "    n_te = n - n_tr\n",
        "    EEG_train = EEG_one_class[:n_tr]\n",
        "    EEG_test = EEG_one_class[n_tr:n_tr+n_te]\n",
        "    return EEG_train, EEG_test\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        temp_EEG_ASD = mod_data[subj]['filtered'][band]['EEG_ASD']\n",
        "        temp_filt = mod_data[subj]['filtered'][band]\n",
        "        temp_filt['EEG_ASD_train'], temp_filt['EEG_ASD_test'] = split_EEG_one_class(temp_EEG_ASD, 0.8)\n",
        "        print(temp_filt['EEG_ASD_test'])\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data1[subj]['filtered'].keys():\n",
        "        temp_EEG_TD = mod_data1[subj]['filtered'][band]['EEG_TD']\n",
        "        temp_filt = mod_data1[subj]['filtered'][band]\n",
        "        temp_filt['EEG_TD_train'], temp_filt['EEG_TD_test'] = split_EEG_one_class(temp_EEG_TD, 0.8)\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data[subj]['CSP'] = {}\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data1[subj]['CSP'] = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.covariance import GraphicalLasso, GraphicalLassoCV\n",
        "def compute_cov(EEG_data):\n",
        "    cov = []\n",
        "    alpha = 0.1\n",
        "    for i in range(EEG_data.shape[0]):\n",
        "        cov.append(EEG_data[i]@EEG_data[i].T/np.trace(EEG_data[i]@EEG_data[i].T))\n",
        "    cov = np.mean(np.array(cov), 0)\n",
        "    return cov\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        temp_band = mod_data[subj]['CSP'][band] = {}\n",
        "        temp_band1 = mod_data1[subj]['CSP'][band] = {}\n",
        "        temp_band['cov_ASD'] = compute_cov(mod_data[subj]['filtered'][band]['EEG_ASD_train'])\n",
        "        temp_band1['cov_TD'] = compute_cov(mod_data1[subj]['filtered'][band]['EEG_TD_train'])\n",
        "        temp_band['cov_comp'] = temp_band['cov_ASD'] + temp_band1['cov_TD']\n",
        "        print(temp_band['cov_comp'])"
      ],
      "metadata": {
        "id": "PCgGILOtt-tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZeybL4XXl8J"
      },
      "outputs": [],
      "source": [
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        mod_data[subj]['CSP'][band]['whitening'] = {}\n",
        "from scipy.linalg import sqrtm\n",
        "from scipy.linalg import inv\n",
        "def decompose_cov(avg_cov):\n",
        "    λ, V = np.linalg.eig(avg_cov)\n",
        "    λ_dsc = np.sort(λ)[::-1]\n",
        "    idx_dsc = np.argsort(λ)[::-1]\n",
        "    V_dsc = V[:, idx_dsc]\n",
        "    λ_dsc = np.diag(λ_dsc)\n",
        "    return λ_dsc, V_dsc\n",
        "def white_matrix(λ_dsc, V_dsc):\n",
        "    λ_dsc_sqr = sqrtm(inv(λ_dsc))\n",
        "    P = (λ_dsc_sqr)@(V_dsc.T)\n",
        "    return P\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        temp_whitening = mod_data[subj]['CSP'][band]['whitening']\n",
        "        temp_cov = mod_data[subj]['CSP'][band]['cov_comp']\n",
        "        temp_whitening['eigval'], temp_whitening['eigvec'] = decompose_cov(temp_cov)\n",
        "        temp_whitening['P'] = white_matrix(temp_whitening['eigval'], temp_whitening['eigvec'])\n",
        "        print(temp_whitening['eigvec'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_RQ2AljX7oE"
      },
      "outputs": [],
      "source": [
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        mod_data[subj]['CSP'][band]['S_ASD'] = {}\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data1[subj]['filtered'].keys():\n",
        "        mod_data1[subj]['CSP'][band]['S_TD'] = {}\n",
        "def compute_S(avg_Cov, white):\n",
        "    S = white@avg_Cov@white.T\n",
        "    return S\n",
        "def decompose_S(S_one_class, order='descending'):\n",
        "    λ, B = np.linalg.eig(S_one_class)\n",
        "    if order == 'ascending':\n",
        "        idx = λ.argsort()\n",
        "    elif order == 'descending':\n",
        "        idx = λ.argsort()[::-1]\n",
        "    else:\n",
        "        print('Wrong order input')\n",
        "    λ = λ[idx]\n",
        "    B = B[:, idx]\n",
        "    return B, λ\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        temp_P = mod_data[subj]['CSP'][band]['whitening']['P']\n",
        "        Cl = mod_data[subj]['CSP'][band]['cov_ASD']\n",
        "        temp_Sl = mod_data[subj]['CSP'][band]['S_ASD']\n",
        "        Sl = compute_S(Cl, temp_P)\n",
        "        temp_Sl['eigvec'], temp_Sl['eigval'] = decompose_S(Sl, 'descending')\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data1[subj]['filtered'].keys():\n",
        "        temp_P = mod_data[subj]['CSP'][band]['whitening']['P']\n",
        "        Cr = mod_data1[subj]['CSP'][band]['cov_TD']\n",
        "        temp_Sr = mod_data1[subj]['CSP'][band]['S_TD']\n",
        "        Sr = compute_S(Cr, temp_P)\n",
        "        temp_Sr['eigvec'], temp_Sr['eigval'] = decompose_S(Sr, 'ascending')\n",
        "        print(temp_Sr['eigval'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kx_jQB3Zglq"
      },
      "outputs": [],
      "source": [
        "def spatial_filter(B, P):\n",
        "    return (B.T@P)\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        temp_eigvec = mod_data[subj]['CSP'][band]['S_ASD']['eigvec']\n",
        "        temp_P = mod_data[subj]['CSP'][band]['whitening']['P']\n",
        "        mod_data[subj]['CSP'][band]['W'] = spatial_filter(temp_eigvec, temp_P)\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data1[subj]['filtered'].keys():\n",
        "        temp_eigvec = mod_data1[subj]['CSP'][band]['S_TD']['eigvec']\n",
        "        temp_P = mod_data[subj]['CSP'][band]['whitening']['P']\n",
        "        mod_data1[subj]['CSP'][band]['W'] = spatial_filter(temp_eigvec, temp_P)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa0MK_scczMo"
      },
      "outputs": [],
      "source": [
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data[subj]['train'] = {}\n",
        "    mod_data[subj]['test'] = {}\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        mod_data[subj]['train'][band] = {}\n",
        "        mod_data[subj]['test'][band] = {}\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data1[subj]['train'] = {}\n",
        "    mod_data1[subj]['test'] = {}\n",
        "    for band in mod_data1[subj]['filtered'].keys():\n",
        "        mod_data1[subj]['train'][band] = {}\n",
        "        mod_data1[subj]['test'][band] = {}\n",
        "m = 2\n",
        "def compute_Z(W, E, m):\n",
        "    Z = []\n",
        "    W = np.delete(W, np.s_[m:-m:], 0)\n",
        "    for i in range(E.shape[0]):\n",
        "        Z.append(W @ E[i])\n",
        "    return np.array(Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2bM4Ee3ypX3"
      },
      "outputs": [],
      "source": [
        "def feat_vector(Z):\n",
        "    feat = []\n",
        "    for i in range(Z.shape[0]):\n",
        "        peak_to_peak = np.ptp(Z[i], axis=1)  # Calculate peak-to-peak amplitude\n",
        "        peak_to_peak_sum = np.sum(peak_to_peak)\n",
        "        feat.append(np.log10(peak_to_peak / peak_to_peak_sum))\n",
        "    return np.array(feat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        temp_W = mod_data[subj]['CSP'][band]['W']\n",
        "        print(len(temp_W))\n",
        "        temp_EEG_ASD = mod_data[subj]['filtered'][band]['ASD']\n",
        "        print(len(temp_EEG_ASD))\n",
        "        mod_data[subj]['train'][band]['Z_ASD'] = compute_Z(temp_W, temp_EEG_ASD, m)\n",
        "        mod_data[subj]['train'][band]['feat_ASD'] = feat_vector(mod_data[subj]['train'][band]['Z_ASD'])\n",
        "        ASD_label = np.zeros([len(mod_data[subj]['train'][band]['feat_ASD']), 1])\n",
        "    for band in mod_data1[subj]['filtered'].keys():\n",
        "        temp_W1 = mod_data1[subj]['CSP'][band]['W']\n",
        "        temp_EEG_ = mod_data1[subj]['filtered'][band]['TD']\n",
        "        mod_data1[subj]['train'][band]['Z_TD'] = compute_Z(temp_W1, temp_EEG_TD, m)\n",
        "        mod_data1[subj]['train'][band]['feat_TD'] = feat_vector(mod_data1[subj]['train'][band]['Z_TD'])\n",
        "        TD_label = np.ones([len(mod_data1[subj]['train'][band]['feat_TD']), 1])\n",
        "        ASD  = np.c_[mod_data[subj]['train'][band]['feat_ASD'], ASD_label]\n",
        "        TD  = np.c_[mod_data1[subj]['train'][band]['feat_TD'], TD_label]\n",
        "        mod_data[subj]['train'][band]['feat_train'] = np.vstack([ASD, TD])\n",
        "        np.random.shuffle(mod_data[subj]['train'][band]['feat_train'])"
      ],
      "metadata": {
        "id": "11fpFJtcWpp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    feat_ASD_all = []\n",
        "    feat_TD_all = []\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        feat_ASD = mod_data[subj]['train'][band]['feat_ASD']\n",
        "        feat_ASD_all.append(feat_ASD)\n",
        "        feat_TD = mod_data1[subj]['train'][band]['feat_TD']\n",
        "        feat_TD_all.append(feat_TD)\n",
        "    merge_ASD = np.zeros(feat_ASD_all[0].shape)\n",
        "    for i in feat_ASD_all:\n",
        "        merge_ASD = np.concatenate([merge_ASD, i], axis=1)\n",
        "    merge_TD = np.zeros(feat_TD_all[0].shape)\n",
        "    for i in feat_TD_all:\n",
        "        merge_TD = np.concatenate([merge_TD, i], axis=1)\n",
        "    true_ASD = np.zeros([merge_ASD.shape[0], 1])\n",
        "    true_TD = np.ones([merge_TD.shape[0], 1])\n",
        "    ASD = np.hstack([merge_ASD, true_ASD])\n",
        "    TD = np.hstack([merge_TD, true_TD])\n",
        "    train_feat = np.vstack([ASD, TD])\n",
        "    np.random.shuffle(train_feat)\n",
        "    mod_data[subj]['train']['all_band'] = train_feat\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif  # You can use f_classif for ANOVA F-statistic as an example\n",
        "def calculate_correlation_coefficient(X, y):\n",
        "    correlation_coefficients = []\n",
        "    for feature in range(X.shape[1]):\n",
        "        correlation, _ = pearsonr(X[:, feature], y)\n",
        "        correlation_coefficients.append(correlation)\n",
        "    return np.array(correlation_coefficients)\n",
        "all_subjects_data = []\n",
        "columns = ['Subject'] + [f'Feature_{i+1}' for i in range(4)] + ['Label']\n",
        "\n",
        "for i in range(1, 11):  # Loop through 10 subjects\n",
        "    subj = subjectCounter(i)  # Get subject index or identifier\n",
        "    X_train = mod_data[subj]['train']['all_band'][:, :-1]  # Features\n",
        "    y_train = mod_data[subj]['train']['all_band'][:, -1]   # Labels\n",
        "\n",
        "    # Initialize dictionary for mutual features if not already done\n",
        "    mod_data[subj]['train']['mutual'] = {}\n",
        "\n",
        "    # Calculate correlation coefficients for feature selection\n",
        "    correlation_coefficients = calculate_correlation_coefficient(X_train, y_train)\n",
        "\n",
        "    # Ensure SelectKBest is properly used\n",
        "    select = SelectKBest(score_func=lambda X, y: correlation_coefficients, k=4)\n",
        "    X_selected = select.fit_transform(X_train, y_train)\n",
        "\n",
        "    # Save selected features and labels in `mod_data`\n",
        "    mod_data[subj]['train']['mutual']['X'] = X_selected\n",
        "    mod_data[subj]['train']['mutual']['y'] = y_train\n",
        "\n",
        "    # Append data for each row to all_subjects_data\n",
        "    for j in range(X_selected.shape[0]):\n",
        "        row_data = [f'Subject_{subj}'] + X_selected[j, :].tolist() + [y_train[j]]\n",
        "        all_subjects_data.append(row_data)\n",
        "\n",
        "# Create DataFrame from collected data\n",
        "df = pd.DataFrame(all_subjects_data, columns=columns)\n",
        "\n",
        "# Save to Excel\n",
        "output_filename = 'selected_train_features_all_subjects.xlsx'\n",
        "df.to_excel(output_filename, index=False)\n",
        "\n",
        "np.random.seed(42)\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        temp_W = mod_data[subj]['CSP'][band]['W']\n",
        "        temp_EEG_ASD = mod_data[subj]['filtered'][band]['EEG_ASD_test']\n",
        "        mod_data[subj]['test'][band]['Z_ASD'] = compute_Z(temp_W, temp_EEG_ASD, m)\n",
        "        mod_data[subj]['test'][band]['feat_ASD'] = feat_vector(mod_data[subj]['test'][band]['Z_ASD'])\n",
        "        ASD_label = np.zeros([len(mod_data[subj]['test'][band]['feat_ASD']), 1])\n",
        "    for band in mod_data1[subj]['filtered'].keys():\n",
        "        temp_W1 = mod_data1[subj]['CSP'][band]['W']\n",
        "        temp_EEG_TD = mod_data1[subj]['filtered'][band]['EEG_TD_test']\n",
        "        mod_data1[subj]['test'][band]['Z_TD'] = compute_Z(temp_W1, temp_EEG_TD, m)\n",
        "        #print(np.shape(mod_data1[subj]['test'][band]['Z_TD'])[0])\n",
        "\n",
        "        mod_data1[subj]['test'][band]['feat_TD'] = feat_vector(mod_data1[subj]['test'][band]['Z_TD'])\n",
        "        #print(mod_data1[subj]['test'][band]['feat_TD'])\n",
        "        TD_label = np.ones([len(mod_data1[subj]['test'][band]['feat_TD']), 1])\n",
        "\n",
        "        ASD  = np.c_[mod_data[subj]['test'][band]['feat_ASD'], ASD_label]\n",
        "        TD  = np.c_[mod_data1[subj]['test'][band]['feat_TD'], TD_label]\n",
        "\n",
        "        mod_data[subj]['test'][band]['feat_test'] = np.vstack([ASD, TD])\n",
        "\n",
        "        np.random.shuffle(mod_data[subj]['test'][band]['feat_test'])\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "\n",
        "    feat_ASD_all = []\n",
        "    feat_TD_all = []\n",
        "\n",
        "    for band in mod_data[subj]['filtered'].keys():\n",
        "        # Access ASD each band\n",
        "        feat_ASD = mod_data[subj]['test'][band]['feat_ASD']\n",
        "\n",
        "        feat_ASD_all.append(feat_ASD)\n",
        "\n",
        "        # Access TD each band\n",
        "        feat_TD = mod_data1[subj]['test'][band]['feat_TD']\n",
        "\n",
        "        feat_TD_all.append(feat_TD)\n",
        "\n",
        "    # MERGING (Need to find more efficient method)\n",
        "    # ASD\n",
        "    merge_ASD = np.zeros(feat_ASD_all[0].shape)\n",
        "\n",
        "    for i in feat_ASD_all:\n",
        "        merge_ASD = np.concatenate([merge_ASD, i], axis=1)\n",
        "\n",
        "    # Delete initial zeros\n",
        "    merge_ASD = np.delete(merge_ASD, np.s_[:2*m], axis=1)\n",
        "\n",
        "    # TD\n",
        "    merge_TD = np.zeros(feat_TD_all[0].shape)\n",
        "\n",
        "    for i in feat_TD_all:\n",
        "        merge_TD = np.concatenate([merge_TD, i], axis=1)\n",
        "\n",
        "    # Delete initial zeros\n",
        "    merge_TD = np.delete(merge_TD, np.s_[:2*m], axis=1)\n",
        "\n",
        "    # TRUE LABEL\n",
        "    true_ASD = np.zeros([merge_ASD.shape[0], 1])\n",
        "    true_TD = np.ones([merge_TD.shape[0], 1])\n",
        "\n",
        "    # FEATURE + TRUE LABEL\n",
        "    ASD = np.hstack([merge_ASD, true_ASD])\n",
        "    TD = np.hstack([merge_TD, true_TD])\n",
        "\n",
        "    # MERGE ASD AND TD\n",
        "    test_feat = np.vstack([ASD, TD])\n",
        "\n",
        "    np.random.shuffle(test_feat)\n",
        "\n",
        "    mod_data[subj]['test']['all_band'] = test_feat\n",
        "#print(len(test_feat))\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif# You can use f_classif for ANOVA F-statistic as an example\n",
        "all_subjects_data = []\n",
        "columns = ['Subject'] + [f'Feature_{i+1}' for i in range(4)] + ['Label']\n",
        "# Define a function to calculate the Pearson correlation coefficient\n",
        "def calculate_correlation_coefficient(X, y):\n",
        "    correlation_coefficients = []\n",
        "    for feature in range(X.shape[1]):\n",
        "        correlation, _ = pearsonr(X[:, feature], y)\n",
        "        correlation_coefficients.append(correlation)\n",
        "    return np.array(correlation_coefficients)\n",
        "for i in range(1, 11):\n",
        "    subj = subjectCounter(i)\n",
        "\n",
        "    X_test = mod_data[subj]['test']['all_band'][:, :-1]\n",
        "    y_test = mod_data[subj]['test']['all_band'][:, -1]\n",
        "\n",
        "    # New dictionary to store result\n",
        "    mod_data[subj]['test']['mutual'] = {}\n",
        "\n",
        "    # Calculate the correlation coefficients for all features\n",
        "    correlation_coefficients = calculate_correlation_coefficient(X_test, y_test)\n",
        "    lamda = 0.001\n",
        "    # Use SelectKBest to select the top 4 features based on correlation coefficient\n",
        "    select = SelectKBest(score_func=lambda X, y: correlation_coefficients, k=4)\n",
        "    X_selected = select.fit_transform(X_test, y_test)\n",
        "\n",
        "    mod_data[subj]['test']['mutual']['X'] = X_selected\n",
        "    mod_data[subj]['test']['mutual']['y'] = y_test\n",
        "    for j in range(X_selected.shape[0]):\n",
        "        row_data = [f'Subject_{subj}'] + X_selected[j, :].tolist() + [y_test[j]]\n",
        "        all_subjects_data.append(row_data)\n",
        "\n",
        "# Create DataFrame from collected data\n",
        "df = pd.DataFrame(all_subjects_data, columns=columns)\n",
        "\n",
        "# Save to Excel\n",
        "output_filename = 'selected_test_features_all_subjects.xlsx'\n",
        "df.to_excel(output_filename, index=False)"
      ],
      "metadata": {
        "id": "_AIWw7GlWaKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Classifier with Data from Excel for a Single Subject\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load data for a single subject from Excel files\n",
        "train_df = pd.read_excel('selected_train_features_all_subjects.xlsx')\n",
        "test_df = pd.read_excel('selected_test_features_all_subjects.xlsx')\n",
        "\n",
        "# Separate features and labels (assuming the last column is the label)\n",
        "X_train = train_df.iloc[:, 1:-1].values  # All columns except the last as features\n",
        "y_train = train_df.iloc[:, -1].values   # Last column as label\n",
        "X_test = test_df.iloc[:, :-1].values\n",
        "y_test = test_df.iloc[:, -1].values\n",
        "\n",
        "# Create an SVM classifier with RBF kernel\n",
        "svm_classifier = SVC(kernel='rbf', C=1)\n",
        "\n",
        "# Train the classifier on the entire training set\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Test the classifier on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "sensitivity_recall = recall_score(y_test, y_pred)\n",
        "specificity = recall_score(y_test, y_pred, pos_label=0)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Test Set Accuracy:\", test_accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Sensitivity (Recall):\", sensitivity_recall)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "zb0FYw-2_o3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply neural network, QSVM and QNN for C4 and Cz combination"
      ],
      "metadata": {
        "id": "17KEIabsM1aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#import pennylane as qml\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn import metrics\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "# Load data for a single subject from Excel files\n",
        "train_df = pd.read_excel('selected_train_features_all_subjects.xlsx')\n",
        "test_df = pd.read_excel('selected_test_features_all_subjects.xlsx')\n",
        "\n",
        "# Separate features and labels (assuming the last column is the label)\n",
        "X_train = train_df.iloc[:, 1:-1].values  # All columns except the first (subject) and last as features\n",
        "y_train = train_df.iloc[:, -1].values   # Last column as label\n",
        "X_test = test_df.iloc[:, 1:-1].values\n",
        "y_test = test_df.iloc[:, -1].values\n",
        "\n",
        "# Defining the model\n",
        "model = keras.Sequential([\n",
        "keras.layers.Dense(32, input_shape=(4,), activation='relu'),\n",
        "keras.layers.Dense(16, activation = 'relu'),\n",
        "keras.layers.Dense(2, activation = 'sigmoid')\n",
        "])\n",
        "# compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=10)\n",
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "precision = precision_score(y_test, y_pred_binary)\n",
        "sensitivity_recall = recall_score(y_test, y_pred_binary)\n",
        "specificity = recall_score(y_test, y_pred_binary, pos_label=0)\n",
        "f1 = f1_score(y_test, y_pred_binary)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_binary))\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Sensitivity (Recall):\", sensitivity_recall)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "C7qu02FVgOj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane"
      ],
      "metadata": {
        "id": "iG-st06v928u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantum SVM Classifier with Data from Excel for a Single Subject\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pennylane as qml\n",
        "from pennylane.templates.embeddings import AmplitudeEmbedding\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load data for a single subject from Excel files\n",
        "train_df = pd.read_excel('selected_train_features_all_subjects.xlsx')\n",
        "test_df = pd.read_excel('selected_test_features_all_subjects.xlsx')\n",
        "\n",
        "# Separate features and labels (assuming the last column is the label)\n",
        "X_train = train_df.iloc[:, 1:-1].values  # All columns except the first (subject) and last as features\n",
        "y_train = train_df.iloc[:, -1].values   # Last column as label\n",
        "X_test = test_df.iloc[:, 1:-1].values\n",
        "y_test = test_df.iloc[:, -1].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Quantum SVM setup\n",
        "n_qubits = X_train.shape[1]\n",
        "dev_kernel = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "projector = np.zeros((2 ** n_qubits, 2 ** n_qubits))\n",
        "projector[0, 0] = 1\n",
        "\n",
        "@qml.qnode(dev_kernel, interface=\"autograd\")\n",
        "def kernel(x1, x2):\n",
        "    \"\"\"Quantum kernel function.\"\"\"\n",
        "    AmplitudeEmbedding(x1, wires=range(n_qubits), pad_with=12.0)\n",
        "    qml.adjoint(AmplitudeEmbedding)(x2, wires=range(n_qubits), pad_with=12.0)\n",
        "    return qml.expval(qml.Hermitian(projector, wires=range(n_qubits)))\n",
        "\n",
        "def kernel_matrix(A, B):\n",
        "    \"\"\"Compute kernel matrix for two datasets.\"\"\"\n",
        "    return np.array([[kernel(a, b) for b in B] for a in A])\n",
        "\n",
        "# Train Quantum SVM using custom kernel\n",
        "quantum_svm = SVC(kernel=lambda A, B: kernel_matrix(A, B)).fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier\n",
        "y_pred = quantum_svm.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "sensitivity_recall = recall_score(y_test, y_pred)\n",
        "specificity = recall_score(y_test, y_pred, pos_label=0)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Test Set Accuracy:\", test_accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Sensitivity (Recall):\", sensitivity_recall)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "id": "0sK0ODF--QfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pennylane as qml\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pennylane as qml\n",
        "from pennylane.templates import AngleEmbedding, StronglyEntanglingLayers, AmplitudeEmbedding\n",
        "from pennylane.operation import Tensor\n",
        "import pennylane as qml\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "np.random.seed(42)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "# Load data for a single subject from Excel files\n",
        "train_df = pd.read_excel('selected_train_features_all_subjects.xlsx')\n",
        "test_df = pd.read_excel('selected_test_features_all_subjects.xlsx')\n",
        "X_train = train_df.iloc[:, 1:-1].values  # All columns except the first (subject) and last as features\n",
        "y_train = train_df.iloc[:, -1].values   # Last column as label\n",
        "X_test = test_df.iloc[:, 1:-1].values\n",
        "y_test = test_df.iloc[:, -1].values\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "layer_1 = tf.keras.layers.Dense(2)\n",
        "layer_2 = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
        "\n",
        "model = tf.keras.Sequential([layer_1, layer_2])\n",
        "model.compile(loss=\"binary_crossentropy\")\n",
        "\n",
        "n_qubits = 2\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def qnode(inputs, weights):\n",
        "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), pad_with=2)\n",
        "    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
        "n_layers = 6\n",
        "weight_shapes = {\"weights\": (n_layers, n_qubits)}\n",
        "# re-define the layers\n",
        "clayer_1 =  tf.keras.layers.Dense(4)\n",
        "dropout_layer = tf.keras.layers.Dropout(0.2)\n",
        "qlayer_1 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\n",
        "qlayer_2 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\n",
        "clayer_2 = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
        "\n",
        "\n",
        "# Check the first layer's input shape\n",
        "#print(f\"clayer_1 input shape: {clayer_1.input_shape}\")\n",
        "# construct the model\n",
        "inputs = tf.keras.Input(shape=(4,))\n",
        "x = clayer_1(inputs)\n",
        "x = dropout_layer(x)\n",
        "x_1, x_2 = tf.split(x, 2, axis=1)\n",
        "x_1 = qlayer_1(x_1)\n",
        "x_2 = qlayer_2(x_2)\n",
        "x = tf.concat([x_1, x_2], axis=1)\n",
        "outputs = clayer_2(x)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "#model.add(Dropout(0.5))\n",
        "model.compile(opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "fitting = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.25, verbose=2)\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(fitting.history['loss'])\n",
        "plt.plot(fitting.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(fitting.history['accuracy'])\n",
        "plt.plot(fitting.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Assuming 'model' is your trained model\n",
        "# Assuming 'X_test' and 'y_test' are your test data and labels\n",
        "\n",
        "# Predict the labels for test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_class = np.argmax(y_pred, axis=1)\n",
        "\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred_class)\n",
        "print(accuracy)\n",
        "# Calculate precision, recall, f1-score, and support\n",
        "report = classification_report(y_true, y_pred_class)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "QDxHjQ4pftl2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-jupy",
      "language": "python",
      "name": "tf-jupy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}